
@inproceedings{schumacher_clinical_2020,
	location = {Online},
	title = {Clinical Concept Linking with Contextualized Neural Representations},
	url = {https://www.aclweb.org/anthology/2020.acl-main.760},
	doi = {10.18653/v1/2020.acl-main.760},
	eventtitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	pages = {8585--8592},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Schumacher, Elliot and Mulyar, Andriy and Dredze, Mark},
	urldate = {2022-10-04},
	date = {2020},
	langid = {english},
	file = {Full Text:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/YY98RK5F/Schumacher et al. - 2020 - Clinical Concept Linking with Contextualized Neura.pdf:application/pdf}
}

@misc{dong_ontology-based_2022,
	title = {Ontology-Based and Weakly Supervised Rare Disease Phenotyping from Clinical Notes},
	url = {http://arxiv.org/abs/2205.05656},
	abstract = {Computational text phenotyping is the practice of identifying patients with certain disorders and traits from clinical notes. Rare diseases are challenging to be identified due to few cases available for machine learning and the need for data annotation from domain experts. We propose a method using ontologies and weak supervision, with recent pre-trained contextual representations from Bi-directional Transformers (e.g. {BERT}). The ontology-based framework includes two steps: (i) Text-to-{UMLS}, extracting phenotypes by contextually linking mentions to concepts in Unified Medical Language System ({UMLS}), with a Named Entity Recognition and Linking ({NER}+L) tool, {SemEHR}, and weak supervision with customised rules and contextual mention representation; (ii) {UMLS}-to-{ORDO}, matching {UMLS} concepts to rare diseases in Orphanet Rare Disease Ontology ({ORDO}). The weakly supervised approach is proposed to learn a phenotype confirmation model to improve Text-to-{UMLS} linking, without annotated data from domain experts. We evaluated the approach on three clinical datasets, {MIMIC}-{III} discharge summaries, {MIMIC}-{III} radiology reports, and {NHS} Tayside brain imaging reports from two institutions in the {US} and the {UK}, with annotations. Our best weakly supervised method achieved 81.4\% precision and 91.4\% recall on extracting rare disease {UMLS} phenotypes from the annotated {MIMIC}-{III} discharge summaries. Results on radiology reports from {MIMIC}-{III} and {NHS} Tayside were consistent with the discharge summaries. The overall pipeline processing clinical notes can extract rare disease cases, mostly uncaptured in structured data (manually assigned {ICD} codes). We discuss the usefulness of the weak supervision approach and propose directions for future studies.},
	number = {{arXiv}:2205.05656},
	publisher = {{arXiv}},
	author = {Dong, Hang and Suárez-Paniagua, Víctor and Zhang, Huayu and Wang, Minhong and Casey, Arlene and Davidson, Emma and Chen, Jiaoyan and Alex, Beatrice and Whiteley, William and Wu, Honghan},
	urldate = {2022-10-04},
	date = {2022-09-09},
	eprinttype = {arxiv},
	eprint = {2205.05656 [cs]},
	keywords = {68T50 (Primary), 68T30 (Secondary), Computer Science - Computation and Language, I.2.7, J.3},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/2JEPHNQP/Dong et al. - 2022 - Ontology-Based and Weakly Supervised Rare Disease .pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/I2MMM5L6/2205.html:text/html}
}

@misc{de_cao_autoregressive_2021,
	title = {Autoregressive Entity Retrieval},
	url = {http://arxiv.org/abs/2010.00904},
	abstract = {Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. Current approaches can be understood as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach has several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose {GENRE}, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion. This mitigates the aforementioned technical issues since: (i) the autoregressive formulation directly captures relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the softmax loss is computed without subsampling negative data. We experiment with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their names. Code and pre-trained models at https://github.com/facebookresearch/{GENRE}.},
	number = {{arXiv}:2010.00904},
	publisher = {{arXiv}},
	author = {De Cao, Nicola and Izacard, Gautier and Riedel, Sebastian and Petroni, Fabio},
	urldate = {2022-10-04},
	date = {2021-03-24},
	eprinttype = {arxiv},
	eprint = {2010.00904 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/J55IZ3SJ/De Cao et al. - 2021 - Autoregressive Entity Retrieval.pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/65J96GAA/2010.html:text/html}
}

@misc{huang_clinicalbert_2020,
	title = {{ClinicalBERT}: Modeling Clinical Notes and Predicting Hospital Readmission},
	url = {http://arxiv.org/abs/1904.05342},
	shorttitle = {{ClinicalBERT}},
	abstract = {Clinical notes contain information about patients that goes beyond structured data like lab values and medications. However, clinical notes have been underused relative to structured data, because notes are high-dimensional and sparse. This work develops and evaluates representations of clinical notes using bidirectional transformers ({ClinicalBERT}). {ClinicalBERT} uncovers high-quality relationships between medical concepts as judged by humans. {ClinicalBert} outperforms baselines on 30-day hospital readmission prediction using both discharge summaries and the first few days of notes in the intensive care unit. Code and model parameters are available.},
	number = {{arXiv}:1904.05342},
	publisher = {{arXiv}},
	author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
	urldate = {2022-10-04},
	date = {2020-11-28},
	eprinttype = {arxiv},
	eprint = {1904.05342 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/GGKCNTB2/Huang et al. - 2020 - ClinicalBERT Modeling Clinical Notes and Predicti.pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/QXNMG87D/1904.html:text/html}
}

@article{sevgili_neural_2022,
	title = {Neural Entity Linking: A Survey of Models Based on Deep Learning},
	volume = {13},
	issn = {22104968, 15700844},
	url = {http://arxiv.org/abs/2006.00575},
	doi = {10.3233/SW-222986},
	shorttitle = {Neural Entity Linking},
	abstract = {This survey presents a comprehensive description of recent neural entity linking ({EL}) systems developed since 2015 as a result of the "deep learning revolution" in natural language processing. Its goal is to systemize design features of neural entity linking systems and compare their performance to the remarkable classic methods on common benchmarks. This work distills a generic architecture of a neural {EL} system and discusses its components, such as candidate generation, mention-context encoding, and entity ranking, summarizing prominent methods for each of them. The vast variety of modifications of this general architecture are grouped by several common themes: joint entity mention detection and disambiguation, models for global linking, domain-independent techniques including zero-shot and distant supervision methods, and cross-lingual approaches. Since many neural models take advantage of entity and mention/context embeddings to represent their meaning, this work also overviews prominent entity embedding techniques. Finally, the survey touches on applications of entity linking, focusing on the recently emerged use-case of enhancing deep pre-trained masked language models based on the Transformer architecture.},
	pages = {527--570},
	number = {3},
	journaltitle = {Semantic Web},
	shortjournal = {{SW}},
	author = {Sevgili, Ozge and Shelmanov, Artem and Arkhipov, Mikhail and Panchenko, Alexander and Biemann, Chris},
	urldate = {2022-10-04},
	date = {2022-04-06},
	eprinttype = {arxiv},
	eprint = {2006.00575 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/GDIXUIA7/Sevgili et al. - 2022 - Neural Entity Linking A Survey of Models Based on.pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/ZCY7FZCY/2006.html:text/html}
}

@misc{du_entity_2022,
	title = {Entity Tagging: Extracting Entities in Text Without Mention Supervision},
	url = {http://arxiv.org/abs/2209.06148},
	shorttitle = {Entity Tagging},
	abstract = {Detection and disambiguation of all entities in text is a crucial task for a wide range of applications. The typical formulation of the problem involves two stages: detect mention boundaries and link all mentions to a knowledge base. For a long time, mention detection has been considered as a necessary step for extracting all entities in a piece of text, even if the information about mention spans is ignored by some downstream applications that merely focus on the set of extracted entities. In this paper we show that, in such cases, detection of mention boundaries does not bring any considerable performance gain in extracting entities, and therefore can be skipped. To conduct our analysis, we propose an "Entity Tagging" formulation of the problem, where models are evaluated purely on the set of extracted entities without considering mentions. We compare a state-of-the-art mention-aware entity linking solution against {GET}, a mention-agnostic sequence-to-sequence model that simply outputs a list of disambiguated entities given an input context. We find that these models achieve comparable performance when trained both on a fully and partially annotated dataset across multiple benchmarks, demonstrating that {GET} can extract disambiguated entities with strong performance without explicit mention boundaries supervision.},
	number = {{arXiv}:2209.06148},
	publisher = {{arXiv}},
	author = {Du, Christina and Popat, Kashyap and Martin, Louis and Petroni, Fabio},
	urldate = {2022-10-04},
	date = {2022-09-13},
	eprinttype = {arxiv},
	eprint = {2209.06148 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/W9ARKZC2/Du et al. - 2022 - Entity Tagging Extracting Entities in Text Withou.pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/HK6ZYV9J/2209.html:text/html}
}

@inproceedings{lin_entitybert_2021,
	location = {Online},
	title = {{EntityBERT}: Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain},
	url = {https://www.aclweb.org/anthology/2021.bionlp-1.21},
	doi = {10.18653/v1/2021.bionlp-1.21},
	shorttitle = {{EntityBERT}},
	eventtitle = {Proceedings of the 20th Workshop on Biomedical Language Processing},
	pages = {191--201},
	booktitle = {Proceedings of the 20th Workshop on Biomedical Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chen and Miller, Timothy and Dligach, Dmitriy and Bethard, Steven and Savova, Guergana},
	urldate = {2022-10-04},
	date = {2021},
	langid = {english},
	file = {Full Text:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/XSERIJNR/Lin et al. - 2021 - EntityBERT Entity-centric Masking Strategy for Mo.pdf:application/pdf}
}

@misc{li_effective_2022,
	title = {Effective Few-Shot Named Entity Linking by Meta-Learning},
	url = {http://arxiv.org/abs/2207.05280},
	abstract = {Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base, which is significant and fundamental for various downstream applications, e.g., knowledge base completion, question answering, and information extraction. While great efforts have been devoted to this task, most of these studies follow the assumption that large-scale labeled data is available. However, when the labeled data is insufficient for specific domains due to labor-intensive annotation work, the performance of existing algorithms will suffer an intolerable decline. In this paper, we endeavor to solve the problem of few-shot entity linking, which only requires a minimal amount of in-domain labeled data and is more practical in real situations. Specifically, we firstly propose a novel weak supervision strategy to generate non-trivial synthetic entity-mention pairs based on mention rewriting. Since the quality of the synthetic data has a critical impact on effective model training, we further design a meta-learning mechanism to assign different weights to each synthetic entity-mention pair automatically. Through this way, we can profoundly exploit rich and precious semantic information to derive a well-trained entity linking model under the few-shot setting. The experiments on real-world datasets show that the proposed method can extensively improve the state-of-the-art few-shot entity linking model and achieve impressive performance when only a small amount of labeled data is available. Moreover, we also demonstrate the outstanding ability of the model's transferability.},
	number = {{arXiv}:2207.05280},
	publisher = {{arXiv}},
	author = {Li, Xiuxing and Li, Zhenyu and Zhang, Zhengyan and Liu, Ning and Yuan, Haitao and Zhang, Wei and Liu, Zhiyuan and Wang, Jianyong},
	urldate = {2022-10-04},
	date = {2022-07-19},
	eprinttype = {arxiv},
	eprint = {2207.05280 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/N4X7AXMY/Li et al. - 2022 - Effective Few-Shot Named Entity Linking by Meta-Le.pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/IZ73JB59/2207.html:text/html}
}

@article{shen_entity_2015,
	title = {Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions},
	volume = {27},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/6823700/},
	doi = {10.1109/TKDE.2014.2327028},
	shorttitle = {Entity Linking with a Knowledge Base},
	pages = {443--460},
	number = {2},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Shen, Wei and Wang, Jianyong and Han, Jiawei},
	urldate = {2022-10-04},
	date = {2015-02-01}
}

@misc{logeswaran_zero-shot_2019,
	title = {Zero-Shot Entity Linking by Reading Entity Descriptions},
	url = {http://arxiv.org/abs/1906.07348},
	abstract = {We present the zero-shot entity linking task, where mentions must be linked to unseen entities without in-domain labeled data. The goal is to enable robust transfer to highly specialized domains, and so no metadata or alias tables are assumed. In this setting, entities are only identified by text descriptions, and models must rely strictly on language understanding to resolve the new entities. First, we show that strong reading comprehension models pre-trained on large unlabeled data can be used to generalize to unseen entities. Second, we propose a simple and effective adaptive pre-training strategy, which we term domain-adaptive pre-training ({DAP}), to address the domain shift problem associated with linking unseen entities in a new domain. We present experiments on a new dataset that we construct for this task and show that {DAP} improves over strong pre-training baselines, including {BERT}. The data and code are available at https://github.com/lajanugen/zeshel.},
	number = {{arXiv}:1906.07348},
	publisher = {{arXiv}},
	author = {Logeswaran, Lajanugen and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina and Devlin, Jacob and Lee, Honglak},
	urldate = {2022-10-04},
	date = {2019-06-17},
	eprinttype = {arxiv},
	eprint = {1906.07348 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/QHEL7IFQ/Logeswaran et al. - 2019 - Zero-Shot Entity Linking by Reading Entity Descrip.pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/CZHY9KYB/1906.html:text/html}
}

@inproceedings{wang_language_2015,
	location = {Lisbon, Portugal},
	title = {Language and Domain Independent Entity Linking with Quantified Collective Validation},
	url = {http://aclweb.org/anthology/D15-1081},
	doi = {10.18653/v1/D15-1081},
	eventtitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	pages = {695--704},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Han and Zheng, Jin Guang and Ma, Xiaogang and Fox, Peter and Ji, Heng},
	urldate = {2022-10-04},
	date = {2015},
	langid = {english},
	file = {Full Text:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/EYJI9HCN/Wang et al. - 2015 - Language and Domain Independent Entity Linking wit.pdf:application/pdf}
}

@incollection{jose_medlinker_2020,
	location = {Cham},
	title = {{MedLinker}: Medical Entity Linking with Neural Representations and Dictionary Matching},
	volume = {12036},
	isbn = {978-3-030-45441-8 978-3-030-45442-5},
	url = {http://link.springer.com/10.1007/978-3-030-45442-5_29},
	shorttitle = {{MedLinker}},
	pages = {230--237},
	booktitle = {Advances in Information Retrieval},
	publisher = {Springer International Publishing},
	author = {Loureiro, Daniel and Jorge, Alípio Mário},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	urldate = {2022-10-04},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-45442-5_29},
	note = {Series Title: Lecture Notes in Computer Science}
}

@misc{johnson_alistair_mimic-iii_2015,
	title = {{MIMIC}-{III} Clinical Database},
	url = {https://physionet.org/content/mimiciii/1.4/},
	abstract = {{MIMIC}-{III} is a large, freely-available database comprising deidentified
health-related data associated with over forty thousand patients who stayed in
critical care units of the Beth Israel Deaconess Medical Center between 2001
and 2012. The database includes information such as demographics, vital sign
measurements made at the bedside ({\textasciitilde}1 data point per hour), laboratory test
results, procedures, medications, caregiver notes, imaging reports, and
mortality (including post-hospital discharge).

{MIMIC} supports a diverse range of analytic studies spanning epidemiology,
clinical decision-rule improvement, and electronic tool development. It is
notable for three factors: it is freely available to researchers worldwide; it
encompasses a diverse and very large population of {ICU} patients; and it
contains highly granular data, including vital signs, laboratory results, and
medications.},
	publisher = {{PhysioNet}},
	author = {Johnson, Alistair and Pollard, Tom and Mark, Roger},
	urldate = {2022-10-04},
	date = {2015},
	doi = {10.13026/C2XW26},
	note = {Version Number: 1.4
Type: dataset}
}

@article{johnson_mimic-iii_2016,
	title = {{MIMIC}-{III}, a freely accessible critical care database},
	volume = {3},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201635},
	doi = {10.1038/sdata.2016.35},
	abstract = {Abstract
            {MIMIC}-{III} (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
	pages = {160035},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Johnson, Alistair E.W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
	urldate = {2022-10-04},
	date = {2016-12-20},
	langid = {english},
	file = {Full Text:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/L8ANVEXC/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care datab.pdf:application/pdf}
}

@article{bodenreider_unified_2004,
	title = {The Unified Medical Language System ({UMLS}): integrating biomedical terminology},
	volume = {32},
	issn = {1362-4962},
	url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkh061},
	doi = {10.1093/nar/gkh061},
	shorttitle = {The Unified Medical Language System ({UMLS})},
	pages = {267D--270},
	number = {90001},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Research},
	author = {Bodenreider, O.},
	urldate = {2022-10-05},
	date = {2004-01-01},
	langid = {english},
	file = {Full Text:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/GQTET3V4/Bodenreider - 2004 - The Unified Medical Language System (UMLS) integr.pdf:application/pdf}
}

@misc{li_effective_2022-1,
	title = {Effective Few-Shot Named Entity Linking by Meta-Learning},
	url = {http://arxiv.org/abs/2207.05280},
	abstract = {Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base, which is significant and fundamental for various downstream applications, e.g., knowledge base completion, question answering, and information extraction. While great efforts have been devoted to this task, most of these studies follow the assumption that large-scale labeled data is available. However, when the labeled data is insufficient for specific domains due to labor-intensive annotation work, the performance of existing algorithms will suffer an intolerable decline. In this paper, we endeavor to solve the problem of few-shot entity linking, which only requires a minimal amount of in-domain labeled data and is more practical in real situations. Specifically, we firstly propose a novel weak supervision strategy to generate non-trivial synthetic entity-mention pairs based on mention rewriting. Since the quality of the synthetic data has a critical impact on effective model training, we further design a meta-learning mechanism to assign different weights to each synthetic entity-mention pair automatically. Through this way, we can profoundly exploit rich and precious semantic information to derive a well-trained entity linking model under the few-shot setting. The experiments on real-world datasets show that the proposed method can extensively improve the state-of-the-art few-shot entity linking model and achieve impressive performance when only a small amount of labeled data is available. Moreover, we also demonstrate the outstanding ability of the model's transferability.},
	number = {{arXiv}:2207.05280},
	publisher = {{arXiv}},
	author = {Li, Xiuxing and Li, Zhenyu and Zhang, Zhengyan and Liu, Ning and Yuan, Haitao and Zhang, Wei and Liu, Zhiyuan and Wang, Jianyong},
	urldate = {2022-10-05},
	date = {2022-07-19},
	eprinttype = {arxiv},
	eprint = {2207.05280 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/6TMUBALQ/Li et al. - 2022 - Effective Few-Shot Named Entity Linking by Meta-Le.pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/7DB4EVIB/2207.html:text/html}
}

@misc{wu_scalable_2020,
	title = {Scalable Zero-shot Entity Linking with Dense Entity Retrieval},
	url = {http://arxiv.org/abs/1911.03814},
	abstract = {This paper introduces a conceptually simple, scalable, and highly effective {BERT}-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. {TACKBP}-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/{BLINK}.},
	number = {{arXiv}:1911.03814},
	publisher = {{arXiv}},
	author = {Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
	urldate = {2022-10-06},
	date = {2020-09-29},
	eprinttype = {arxiv},
	eprint = {1911.03814 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/N5TM3LTV/Wu et al. - 2020 - Scalable Zero-shot Entity Linking with Dense Entit.pdf:application/pdf;arXiv.org Snapshot:/Users/willthompson/OneDrive/OneDrive - Northwestern University/WKT/References/storage/V6758NHE/1911.html:text/html}
}